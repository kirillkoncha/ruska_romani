{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3b57c2",
   "metadata": {},
   "source": [
    "# Разборное разбираем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc5e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "from razdel import tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e4e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kirillkonca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643ca46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = SovietRomaniAnalyzer(mode='strict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328baf5b",
   "metadata": {},
   "source": [
    "# Borrowings Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32460f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93963d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pym2uni = {'masc': 'm',\n",
    "           'femn':'f',\n",
    "           'neut': 'n',\n",
    "            None: '',\n",
    "           'sing': 'sg',\n",
    "           'plur': 'pl',\n",
    "           'NOUN': 'N'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e454af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffs = [(['obl', 'm', 'sg'], 'с'),\n",
    "         (['gen', 'm', 'sg'], 'скирэ'),\n",
    "         (['gen', 'm', 'sg'], 'скир'),\n",
    "         (['gen', 'm', 'sg'], 'скр'),\n",
    "         (['gen', 'f', 'sg'], 'кир'),\n",
    "         (['gen', 'f', 'sg'], 'кр'),\n",
    "         (['dat', 'm', 'sg'], 'скэ'),\n",
    "         (['dat', 'f', 'sg'], 'кэ'),\n",
    "         (['ins', '', 'sg'], 'са'),\n",
    "         (['loc', 'm', 'sg'], 'стэ'),\n",
    "         (['loc', 'f', 'sg'], 'тэ'),\n",
    "         (['abl', 'm', 'sg'], 'стыр'),\n",
    "         (['abl', 'f', 'sg'], 'тыр'),\n",
    "         (['obl', '', 'pl'], 'н'),\n",
    "         (['gen', '', 'pl'], 'нгир'),\n",
    "         (['gen', '', 'pl'], 'нгр'),\n",
    "         (['abl', '', 'pl'], 'ндыр'),\n",
    "         (['dat', '', 'pl'], 'нгэ'),\n",
    "         (['loc', '', 'pl'], 'ндэ'),\n",
    "         (['ins', '', 'pl'], 'нса'),\n",
    "         (['ins', '', 'pl'], 'нца'),\n",
    "         (['voc', '', 'pl'], 'лэ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27099e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem2rom(stem, gramm, word, lemma=''):\n",
    "    stem = stem.replace('жэ', 'же')  # жертва\n",
    "    stem = stem.replace('жы', 'жи')\n",
    "    stem = stem.replace('шы', 'ши')\n",
    "\n",
    "    ru_analys = morph.parse(stem)[0]\n",
    "\n",
    "    notfakedict = str(ru_analys.methods_stack[0][0]) != 'FakeDictionary()'\n",
    "    whole_an = len(ru_analys.methods_stack[0][1]) == len(stem)\n",
    "    name = word.istitle()\n",
    "    good_score = ru_analys.score >= 0.2\n",
    "    \n",
    "    if name or (notfakedict and whole_an and good_score):\n",
    "        if ru_analys.tag.POS == 'NOUN':\n",
    "            trans_ru = ru_analys.normal_form\n",
    "            #trans_en = translator.translate(trans_ru, src='ru').text.lower()\n",
    "            if lemma == '':\n",
    "                lemma = trans_ru\n",
    "            if gramm[1] == '':\n",
    "                gender = pym2uni[ru_analys.tag.gender]\n",
    "            else:\n",
    "                gender = gramm[1]\n",
    "            if gramm[2] == '':\n",
    "                number = pym2uni[ru_analys.tag.number]\n",
    "            else:\n",
    "                number = gramm[2]\n",
    "            wfGlossed = word.lower()\n",
    "            #gloss = trans_en\n",
    "            if gramm[0] != '':\n",
    "                wfGlossed = stem + '-' + word[len(stem):]\n",
    "                #gloss = trans_en + '-' + (gramm[0]).upper() \n",
    "            rom_analys = {'wf': word,\n",
    "                          'lemma': lemma,\n",
    "                          'gramm': ['N',\n",
    "                                    gender,\n",
    "                                    gramm[0],\n",
    "                                    number],\n",
    "                          'wfGlossed': wfGlossed,\n",
    "                          'trans_ru': trans_ru}\n",
    "            return rom_analys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d832c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrowing_an(word):\n",
    "    unknown = {'wf': word,\n",
    "               'lemma': '',\n",
    "               'gramm': [],\n",
    "               'wfGlossed': '',\n",
    "               'gloss': '',\n",
    "               'trans_en': '',\n",
    "               'trans_ru': ''}\n",
    "    word_nojo = word.replace('ё', 'е')\n",
    "\n",
    "    for gramm, suff in suffs:\n",
    "        if word.endswith(suff):\n",
    "            stems = [word[:-len(suff)], word[:-len(suff)-1],\n",
    "                     word_nojo[:-len(suff)], word_nojo[:-len(suff)-1]]\n",
    "            if word[:-len(suff)].endswith('ь'):\n",
    "                stems.append(word[:-len(suff)-2])\n",
    "            for stem in stems:\n",
    "                rom_analys = stem2rom(stem, gramm, word)\n",
    "                if rom_analys != None:\n",
    "                    return rom_analys\n",
    "    \n",
    "    if word.endswith('ё') or word.endswith('о') or word.endswith('э'):\n",
    "        rom_analys = stem2rom(word[:-1], ['', '', 'dir'], word, lemma=word) #вокатив, дир\n",
    "        if rom_analys != None:\n",
    "            return rom_analys\n",
    "\n",
    "    rom_analys = stem2rom(word, ['', '', ''], word)\n",
    "    if rom_analys != None:\n",
    "        return rom_analys\n",
    "    \n",
    "    return unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573bc8d",
   "metadata": {},
   "source": [
    "# Corpus Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd76f1",
   "metadata": {},
   "source": [
    "Read file with pairs. Every text has to be annotated separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "c6e6f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"pairs/zlodei.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08d2a3",
   "metadata": {},
   "source": [
    "Delete not full quatations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "2f9f0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n6/_05m2dl943qgt_yt01lb8h_w0000gn/T/ipykernel_50305/1143294128.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['to'][i] = df['to'][i][:-1]\n",
      "/var/folders/n6/_05m2dl943qgt_yt01lb8h_w0000gn/T/ipykernel_50305/1143294128.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['to'][i] = df['to'][i][0:]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    if str(df['to'][i]) != \"nan\":\n",
    "        if (df['to'][i][-1] == \"»\" or df['to'][i][-1] == \"».\") and df['to'][i][0] != \"«\":\n",
    "            df['to'][i] = df['to'][i][:-1]\n",
    "        if df['to'][i][0] == \"«\" and df['to'][i][-1] != \"»\":\n",
    "            df['to'][i] = df['to'][i][0:]\n",
    "        if df['to'][i][0] == '\"' and df['to'][i][-1] != '\"':\n",
    "            df['to'][i] = df['to'][i][0:]\n",
    "        if df['to'][i][-1] == '\"' and df['to'][i][0] != '\"':\n",
    "            df['to'][i] = df['to'][i][:-1]\n",
    "        if df['to'][i][0] == '(' and df['to'][i][-1] != ')':\n",
    "            df['to'][i] = df['to'][i][0:]\n",
    "        if df['to'][i][-1] == ')' and df['to'][i][0] != '(':\n",
    "            df['to'][i] = df['to'][i][:-1]\n",
    "    if str(df['from'][i]) != \"nan\":\n",
    "        if df['from'][i][-1] == \"»\" and df['from'][i][0] != \"«\":\n",
    "            df['from'][i] = df['from'][i][:-1]\n",
    "        if df['from'][i][0] == \"«\" and df['from'][i][-1] != \"»\":\n",
    "            df['from'][i] = df['from'][i][0:]\n",
    "        if df['from'][i][0] == '\"' and df['from'][i][-1] != '\"':\n",
    "            df['from'][i] = df['from'][i][0:]\n",
    "        if df['from'][i][-1] == '\"' and df['from'][i][0] != '\"':\n",
    "            df['from'][i] = df['from'][i][:-1]\n",
    "        if df['from'][i][0] == '(' and df['from'][i][-1] != ')':\n",
    "            df['from'][i] = df['from'][i][0:]\n",
    "        if df['from'][i][-1] == ')' and df['from'][i][0] != '(':\n",
    "            df['from'][i] = df['from'][i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "ae82db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "d2bae423",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_ru = df['to'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "52947071",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_roma = df['from'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd90b47",
   "metadata": {},
   "source": [
    "Delete byte tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "2a79534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences_roma)):\n",
    "    if str(sentences_roma[i]) != \"nan\":\n",
    "        sentences_roma[i] = sentences_roma[i].replace(\"\\ufeff\", \"\")\n",
    "        sentences_roma[i] = sentences_roma[i].replace(\"\\ufeff \", \"\")\n",
    "        sentences_roma[i] = sentences_roma[i].replace(\"\\xa0\", \"\")\n",
    "        sentences_roma[i] = sentences_roma[i].replace(\"\\xa0 \", \"\")\n",
    "        sentences_roma[i] = sentences_roma[i].replace(\"\\xad\", \"\")\n",
    "        sentences_roma[i] = sentences_roma[i].replace(\"\\xad \", \"\")\n",
    "        sentences_roma[i] = sentences_roma[i].replace(\"\\t\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "2b76a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences_ru)):\n",
    "    if str(sentences_ru[i]) != \"nan\":\n",
    "        sentences_ru[i] = sentences_ru[i].replace(\"\\ufeff\", \"\")\n",
    "        sentences_ru[i] = sentences_ru[i].replace(\"\\ufeff \", \"\")\n",
    "        sentences_ru[i] = sentences_ru[i].replace(\"\\xa0\", \"\")\n",
    "        sentences_ru[i] = sentences_ru[i].replace(\"\\xa0 \", \"\")\n",
    "        sentences_ru[i] = sentences_ru[i].replace(\"\\xad\", \"\")\n",
    "        sentences_ru[i] = sentences_ru[i].replace(\"\\xad \", \"\")\n",
    "        sentences_ru[i] = sentences_ru[i].replace(\"-\\t\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058717fd",
   "metadata": {},
   "source": [
    "Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "50d380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {\"corpus\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "bb1ab3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 676/676 [00:16<00:00, 40.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(sentences_roma))):\n",
    "    if str(sentences_roma[i]) != \"nan\" and str(sentences_roma[i]) != ' ':\n",
    "        toks_in_sents = [word_tokenize(sent) for sent in sent_tokenize(sentences_roma[i])]\n",
    "        analyses = a.analyze_words(toks_in_sents, format=\"json\")\n",
    "        analyses = analyses[0]\n",
    "    else:\n",
    "        analyses = []\n",
    "    roma_words = []\n",
    "    for an in analyses:\n",
    "        words = []\n",
    "        word = an[0]\n",
    "        # Analyses each part of words with hyphens separately\n",
    "        if len(word[\"wf\"].split(\"-\")) > 1:\n",
    "            for w in word[\"wf\"].split(\"-\"):\n",
    "                an_w = a.analyze_words(w, format=\"json\")\n",
    "                if len(an_w[0][\"wf\"]) == 0:\n",
    "                    an_w = [borrowing_an(an_w[0]['wf'])]\n",
    "                for pos_an in an_w:\n",
    "                    words.append(pos_an)\n",
    "            break\n",
    "        # Analyse borrowing if not annotated with uniparser\n",
    "        if len(word[\"lemma\"]) == 0 and word[\"wf\"] not in string.punctuation:\n",
    "            an_w = borrowing_an(word[\"wf\"])\n",
    "            words.append(an_w)\n",
    "        else:\n",
    "            for al in an:\n",
    "                words.append(al)\n",
    "        roma_words.append(words)\n",
    "    for mn in roma_words:\n",
    "        for var in mn:\n",
    "            if \"trans_ru\" not in var:\n",
    "                var[\"trans_ru\"] = \"\"\n",
    "            if \"trans_en\" not in var:\n",
    "                var[\"trans_en\"] = \"\"\n",
    "    ru = sent_tokenize(sentences_ru[i])\n",
    "    ru = word_tokenize(ru[0])\n",
    "    final_dict[\"corpus\"].append({\"sentence_ru\": sentences_ru[i], \"sentence_roma\": sentences_roma[i], \"sentence_id\": i, \"words_roma\": roma_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "bbca041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('zlodei.json', 'w') as f:\n",
    "    f.write(json.dumps(final_dict, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4878f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
